{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Dataset/preprocessed_dataset.csv\")  # Replace with your dataset\n",
    "similarity_matrix = joblib.load(\"similarity.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jolly\n",
      "[nltk_data]     InfoTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jolly\n",
      "[nltk_data]     InfoTech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer that splits on non-alphanumeric characters\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())  # Tokenize & convert to lowercase\n",
    "    \n",
    "    # Get English stop words (with error handling)\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        stop_words = set()  # Empty set if stopwords not available\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "    print(filtered_words)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_product(query, data):\n",
    "    query_words = preprocess_text(query)\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # Use the same tokenizer here instead of word_tokenize\n",
    "        product_words = tokenizer.tokenize(row[\"tags\"].lower())\n",
    "        \n",
    "        if any(word in product_words for word in query_words):  # Match words\n",
    "            return row[\"Product_id\"]\n",
    "    \n",
    "    # If no match is found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation function\n",
    "def recommend_products(product_id, data, similarity_matrix, top_n=5):\n",
    "    index = data[data[\"Product_id\"] == product_id].index[0]\n",
    "    similarity_scores = list(enumerate(similarity_matrix[index]))\n",
    "    similar_products = sorted(similarity_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    recommended_products = [data.iloc[i[0]]['Product_id'] for i in similar_products]\n",
    "    return recommended_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_matching_product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue shirt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m product_id \u001b[38;5;241m=\u001b[39m \u001b[43mfind_matching_product\u001b[49m(user_message, data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_matching_product' is not defined"
     ]
    }
   ],
   "source": [
    "user_message = \"blue shirt\"\n",
    "\n",
    "product_id = find_matching_product(user_message, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11690882"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Jolly InfoTech/nltk_data', 'd:\\\\Python\\\\envs\\\\ml\\\\nltk_data', 'd:\\\\Python\\\\envs\\\\ml\\\\share\\\\nltk_data', 'd:\\\\Python\\\\envs\\\\ml\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Jolly InfoTech\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10504730, 4441307, 10632192, 10636630, 12259184]\n"
     ]
    }
   ],
   "source": [
    "if product_id:\n",
    "    recommendations = recommend_products(product_id, data, similarity_matrix, top_n=5)\n",
    "    print(recommendations)\n",
    "else:\n",
    "    print(\"Sorry, I couldn't find a matching product. Try another query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers/punkt not found.\n",
      "corpora/stopwords not found.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.data import path\n",
    "\n",
    "# Find the paths where nltk stores data\n",
    "nltk_data_path = path[0]\n",
    "\n",
    "# Delete specific datasets\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def delete_nltk_resource(resource_name):\n",
    "    resource_path = os.path.join(nltk_data_path, resource_name)\n",
    "    if os.path.exists(resource_path):\n",
    "        shutil.rmtree(resource_path)\n",
    "        print(f\"Deleted {resource_name}\")\n",
    "    else:\n",
    "        print(f\"{resource_name} not found.\")\n",
    "\n",
    "# Remove stopwords and punkt\n",
    "delete_nltk_resource(\"tokenizers/punkt\")\n",
    "delete_nltk_resource(\"corpora/stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolly InfoTech\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n",
      "C:\\Users\\Jolly InfoTech\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find(\"tokenizers/punkt\"))\n",
    "print(nltk.data.find(\"corpora/stopwords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
