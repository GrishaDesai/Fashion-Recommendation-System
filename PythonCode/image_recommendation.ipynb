{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "class ClothingRecommendationModel:\n",
    "    def __init__(self, input_shape=(224, 224, 3), embedding_dim=256):\n",
    "        \"\"\"\n",
    "        Initialize the clothing recommendation model\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Input image dimensions\n",
    "            embedding_dim (int): Dimension of the image embedding vector\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build the recommendation model architecture\n",
    "        \n",
    "        Returns:\n",
    "            tf.keras.Model: Compiled recommendation model\n",
    "        \"\"\"\n",
    "        # Use pre-trained ResNet50 as base model\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Custom embedding layers\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Final embedding layer\n",
    "        embedding = layers.Dense(\n",
    "            self.embedding_dim, \n",
    "            activation='relu', \n",
    "            name='embedding_layer'\n",
    "        )(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = models.Model(\n",
    "            inputs=base_model.input, \n",
    "            outputs=embedding\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='cosine_similarity'\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, train_data, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the recommendation model\n",
    "        \n",
    "        Args:\n",
    "            train_data (tf.data.Dataset): Training dataset\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \"\"\"\n",
    "        # Data augmentation\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.2),\n",
    "            layers.RandomZoom(0.2),\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            train_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def get_similar_images(self, reference_image, image_database, top_k=5):\n",
    "        \"\"\"\n",
    "        Find similar images based on embedding similarity\n",
    "        \n",
    "        Args:\n",
    "            reference_image (np.array): Input reference image\n",
    "            image_database (list): List of images to compare\n",
    "            top_k (int): Number of similar images to return\n",
    "        \n",
    "        Returns:\n",
    "            list: Indices of top similar images\n",
    "        \"\"\"\n",
    "        # Get embedding for reference image\n",
    "        reference_embedding = self.model.predict(\n",
    "            np.expand_dims(reference_image, axis=0)\n",
    "        )\n",
    "        \n",
    "        # Compute embeddings for database\n",
    "        database_embeddings = self.model.predict(image_database)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = tf.keras.losses.cosine_similarity(\n",
    "            reference_embedding, \n",
    "            database_embeddings\n",
    "        ).numpy()\n",
    "        \n",
    "        # Find top-k most similar images\n",
    "        top_indices = np.argsort(similarities)[0][:top_k]\n",
    "        \n",
    "        return top_indices\n",
    "\n",
    "# Example usage\n",
    "def prepare_dataset(image_directory):\n",
    "    \"\"\"\n",
    "    Prepare image dataset for training\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to clothing image directory\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Prepared training dataset\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        image_directory,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    return train_generator\n",
    "\n",
    "def save_model(self, save_path='clothing_recommendation_model'):\n",
    "    \"\"\"\n",
    "    Save the trained model to local file\n",
    "    \n",
    "    Args:\n",
    "        save_path (str): Directory to save model\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save full model\n",
    "    full_model_path = os.path.join(save_path, 'full_model')\n",
    "    self.model.save(full_model_path)\n",
    "    \n",
    "    # Save weights separately\n",
    "    weights_path = os.path.join(save_path, 'model_weights.weights.h5')\n",
    "    self.model.save_weights(weights_path)\n",
    "    \n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "@classmethod\n",
    "def load_model(cls, load_path='clothing_recommendation_model'):\n",
    "    \"\"\"\n",
    "    Load a previously saved model\n",
    "    \n",
    "    Args:\n",
    "        load_path (str): Directory containing saved model\n",
    "    \n",
    "    Returns:\n",
    "        ClothingRecommendationModel: Loaded model instance\n",
    "    \"\"\"\n",
    "    # Create model instance\n",
    "    model_instance = cls()\n",
    "    \n",
    "    # Try to load full model first\n",
    "    full_model_path = os.path.join(load_path, 'full_model')\n",
    "    weights_path = os.path.join(load_path, 'model_weights.weights.h5')\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load full model\n",
    "        model_instance.model = tf.keras.models.load_model(full_model_path)\n",
    "        print(\"Full model loaded successfully.\")\n",
    "    except:\n",
    "        # If full model loading fails, rebuild and load weights\n",
    "        model_instance.model = model_instance._build_model()\n",
    "        model_instance.model.load_weights(weights_path)\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "    \n",
    "    return model_instance\n",
    "\n",
    "# Main execution example\n",
    "def main():\n",
    "    # Initialize model\n",
    "    recommendation_model = ClothingRecommendationModel()\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_data = prepare_dataset('path/to/clothing/images')\n",
    "    \n",
    "    # Train model\n",
    "    recommendation_model.train(train_data)\n",
    "\n",
    "    # Save the model\n",
    "    recommendation_model.save_model('my_clothing_model')\n",
    "\n",
    "    # Load the model (demonstrates loading functionality)\n",
    "    loaded_model = ClothingRecommendationModel.load_model('my_clothing_model')\n",
    "    \n",
    "    print(\"Clothing Recommendation Model Training Complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Loaded:     Unnamed: 0.2  id   category  \\\n",
      "0             0   0  shapewear   \n",
      "1             1   1    tshirts   \n",
      "2             2   2       tops   \n",
      "3             3   3       tops   \n",
      "4             4   4    tshirts   \n",
      "\n",
      "                                           image_url  Unnamed: 0.1  \\\n",
      "0  https://assets.myntassets.com/h_720,q_90,w_540...           NaN   \n",
      "1  https://assets.myntassets.com/h_720,q_90,w_540...           NaN   \n",
      "2  https://assets.myntassets.com/h_720,q_90,w_540...           NaN   \n",
      "3  https://assets.myntassets.com/h_720,q_90,w_540...           NaN   \n",
      "4  https://assets.myntassets.com/h_720,q_90,w_540...           NaN   \n",
      "\n",
      "   Unnamed: 0              local_path  \n",
      "0         NaN  processed_images/0.jpg  \n",
      "1         NaN  processed_images/1.jpg  \n",
      "2         NaN  processed_images/2.jpg  \n",
      "3         NaN  processed_images/3.jpg  \n",
      "4         NaN  processed_images/4.jpg  \n",
      "[[[[0.9411765  0.9254902  0.88235295]\n",
      "   [0.9411765  0.9254902  0.88235295]\n",
      "   [0.9411765  0.9254902  0.88235295]\n",
      "   ...\n",
      "   [0.9647059  0.9607843  0.94509804]\n",
      "   [0.96862745 0.9647059  0.9490196 ]\n",
      "   [0.96862745 0.9647059  0.9490196 ]]\n",
      "\n",
      "  [[0.9411765  0.9254902  0.88235295]\n",
      "   [0.9411765  0.9254902  0.88235295]\n",
      "   [0.9411765  0.9254902  0.88235295]\n",
      "   ...\n",
      "   [0.9647059  0.9607843  0.94509804]\n",
      "   [0.9647059  0.9607843  0.94509804]\n",
      "   [0.9647059  0.9607843  0.94509804]]\n",
      "\n",
      "  [[0.9372549  0.92156863 0.8784314 ]\n",
      "   [0.9372549  0.92156863 0.8784314 ]\n",
      "   [0.9372549  0.92156863 0.8784314 ]\n",
      "   ...\n",
      "   [0.9647059  0.9607843  0.94509804]\n",
      "   [0.9647059  0.9607843  0.94509804]\n",
      "   [0.9647059  0.9607843  0.94509804]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.92941177 0.90588236 0.85882354]\n",
      "   [0.92941177 0.90588236 0.85882354]\n",
      "   [0.93333334 0.9098039  0.8627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]]\n",
      "\n",
      "  [[0.93333334 0.9098039  0.8627451 ]\n",
      "   [0.9372549  0.9137255  0.8666667 ]\n",
      "   [0.93333334 0.9098039  0.8627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]]\n",
      "\n",
      "  [[0.93333334 0.9098039  0.8627451 ]\n",
      "   [0.9372549  0.9137255  0.8666667 ]\n",
      "   [0.93333334 0.9098039  0.8627451 ]\n",
      "   ...\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]\n",
      "   [0.9490196  0.9372549  0.9019608 ]]]\n",
      "\n",
      "\n",
      " [[[0.7176471  0.7176471  0.7176471 ]\n",
      "   [0.7176471  0.7176471  0.7176471 ]\n",
      "   [0.7137255  0.7137255  0.7137255 ]\n",
      "   ...\n",
      "   [0.7411765  0.74509805 0.7647059 ]\n",
      "   [0.7411765  0.74509805 0.7647059 ]\n",
      "   [0.7411765  0.74509805 0.7647059 ]]\n",
      "\n",
      "  [[0.7176471  0.7176471  0.7176471 ]\n",
      "   [0.72156864 0.72156864 0.72156864]\n",
      "   [0.7176471  0.7176471  0.7176471 ]\n",
      "   ...\n",
      "   [0.7411765  0.74509805 0.7647059 ]\n",
      "   [0.7372549  0.7411765  0.7607843 ]\n",
      "   [0.7372549  0.7411765  0.7607843 ]]\n",
      "\n",
      "  [[0.72156864 0.72156864 0.72156864]\n",
      "   [0.72156864 0.72156864 0.72156864]\n",
      "   [0.72156864 0.72156864 0.72156864]\n",
      "   ...\n",
      "   [0.7411765  0.74509805 0.7647059 ]\n",
      "   [0.7372549  0.7411765  0.7607843 ]\n",
      "   [0.7372549  0.7411765  0.7607843 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7921569  0.7882353  0.78039217]\n",
      "   [0.7921569  0.7882353  0.78039217]\n",
      "   [0.7921569  0.7882353  0.78039217]\n",
      "   ...\n",
      "   [0.7764706  0.7764706  0.7764706 ]\n",
      "   [0.78039217 0.78039217 0.78039217]\n",
      "   [0.78039217 0.78039217 0.78039217]]\n",
      "\n",
      "  [[0.7921569  0.7882353  0.78039217]\n",
      "   [0.7921569  0.7882353  0.78039217]\n",
      "   [0.7882353  0.78431374 0.7764706 ]\n",
      "   ...\n",
      "   [0.78431374 0.78431374 0.78431374]\n",
      "   [0.78431374 0.78431374 0.78431374]\n",
      "   [0.78431374 0.78431374 0.78431374]]\n",
      "\n",
      "  [[0.8039216  0.8        0.7921569 ]\n",
      "   [0.8039216  0.8        0.7921569 ]\n",
      "   [0.8039216  0.8        0.7921569 ]\n",
      "   ...\n",
      "   [0.7921569  0.7921569  0.7921569 ]\n",
      "   [0.7882353  0.7882353  0.7882353 ]\n",
      "   [0.7921569  0.7921569  0.7921569 ]]]\n",
      "\n",
      "\n",
      " [[[0.87058824 0.8784314  0.8745098 ]\n",
      "   [0.87058824 0.8784314  0.8745098 ]\n",
      "   [0.87058824 0.8784314  0.8745098 ]\n",
      "   ...\n",
      "   [0.67058825 0.67058825 0.67058825]\n",
      "   [0.6627451  0.6627451  0.6627451 ]\n",
      "   [0.6627451  0.6627451  0.6627451 ]]\n",
      "\n",
      "  [[0.87058824 0.8784314  0.8745098 ]\n",
      "   [0.87058824 0.8784314  0.8745098 ]\n",
      "   [0.8745098  0.88235295 0.8784314 ]\n",
      "   ...\n",
      "   [0.6745098  0.6745098  0.6745098 ]\n",
      "   [0.67058825 0.67058825 0.67058825]\n",
      "   [0.67058825 0.67058825 0.67058825]]\n",
      "\n",
      "  [[0.8745098  0.88235295 0.8784314 ]\n",
      "   [0.8745098  0.88235295 0.8784314 ]\n",
      "   [0.8784314  0.8862745  0.88235295]\n",
      "   ...\n",
      "   [0.6784314  0.6784314  0.6784314 ]\n",
      "   [0.6745098  0.6745098  0.6745098 ]\n",
      "   [0.67058825 0.67058825 0.67058825]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196 ]\n",
      "   [0.9490196  0.9490196  0.9490196 ]\n",
      "   ...\n",
      "   [0.7764706  0.78431374 0.78039217]\n",
      "   [0.7764706  0.78431374 0.78039217]\n",
      "   [0.77254903 0.78039217 0.7764706 ]]\n",
      "\n",
      "  [[0.9529412  0.9529412  0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196 ]\n",
      "   [0.9529412  0.9529412  0.9529412 ]\n",
      "   ...\n",
      "   [0.78039217 0.7882353  0.78431374]\n",
      "   [0.7764706  0.78431374 0.78039217]\n",
      "   [0.77254903 0.78039217 0.7764706 ]]\n",
      "\n",
      "  [[0.9490196  0.9490196  0.9490196 ]\n",
      "   [0.9529412  0.9529412  0.9529412 ]\n",
      "   [0.9490196  0.9490196  0.9490196 ]\n",
      "   ...\n",
      "   [0.78039217 0.7882353  0.78431374]\n",
      "   [0.7764706  0.78431374 0.78039217]\n",
      "   [0.77254903 0.78039217 0.7764706 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.68235296 0.6862745  0.7019608 ]\n",
      "   ...\n",
      "   [0.69411767 0.69803923 0.7137255 ]\n",
      "   [0.69411767 0.69803923 0.7137255 ]\n",
      "   [0.69411767 0.69803923 0.7137255 ]]\n",
      "\n",
      "  [[0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.68235296 0.6862745  0.7019608 ]\n",
      "   ...\n",
      "   [0.69411767 0.69803923 0.7137255 ]\n",
      "   [0.69803923 0.7019608  0.7176471 ]\n",
      "   [0.69803923 0.7019608  0.7176471 ]]\n",
      "\n",
      "  [[0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.68235296 0.6862745  0.7019608 ]\n",
      "   [0.6862745  0.6901961  0.7058824 ]\n",
      "   ...\n",
      "   [0.69411767 0.69803923 0.7137255 ]\n",
      "   [0.69411767 0.69803923 0.7137255 ]\n",
      "   [0.69411767 0.69803923 0.7137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7019608  0.7058824  0.72156864]\n",
      "   [0.7058824  0.70980394 0.7254902 ]\n",
      "   [0.7019608  0.7058824  0.72156864]\n",
      "   ...\n",
      "   [0.654902   0.65882355 0.6745098 ]\n",
      "   [0.654902   0.65882355 0.6745098 ]\n",
      "   [0.65882355 0.6627451  0.6784314 ]]\n",
      "\n",
      "  [[0.7019608  0.7058824  0.72156864]\n",
      "   [0.7058824  0.70980394 0.7254902 ]\n",
      "   [0.7058824  0.70980394 0.7254902 ]\n",
      "   ...\n",
      "   [0.654902   0.65882355 0.6745098 ]\n",
      "   [0.65882355 0.6627451  0.6784314 ]\n",
      "   [0.65882355 0.6627451  0.6784314 ]]\n",
      "\n",
      "  [[0.7019608  0.7058824  0.72156864]\n",
      "   [0.7019608  0.7058824  0.72156864]\n",
      "   [0.7019608  0.7058824  0.72156864]\n",
      "   ...\n",
      "   [0.654902   0.65882355 0.6745098 ]\n",
      "   [0.654902   0.65882355 0.6745098 ]\n",
      "   [0.654902   0.65882355 0.6745098 ]]]\n",
      "\n",
      "\n",
      " [[[0.84705883 0.84705883 0.85490197]\n",
      "   [0.84705883 0.84705883 0.85490197]\n",
      "   [0.84705883 0.84705883 0.85490197]\n",
      "   ...\n",
      "   [0.78039217 0.78039217 0.7882353 ]\n",
      "   [0.78039217 0.78039217 0.7882353 ]\n",
      "   [0.78039217 0.78039217 0.7882353 ]]\n",
      "\n",
      "  [[0.8509804  0.8509804  0.85882354]\n",
      "   [0.8509804  0.8509804  0.85882354]\n",
      "   [0.8509804  0.8509804  0.85882354]\n",
      "   ...\n",
      "   [0.78431374 0.78431374 0.7921569 ]\n",
      "   [0.78039217 0.78039217 0.7882353 ]\n",
      "   [0.78039217 0.78039217 0.7882353 ]]\n",
      "\n",
      "  [[0.8509804  0.8509804  0.85882354]\n",
      "   [0.8509804  0.8509804  0.85882354]\n",
      "   [0.8509804  0.8509804  0.85882354]\n",
      "   ...\n",
      "   [0.78431374 0.78431374 0.7921569 ]\n",
      "   [0.78431374 0.78431374 0.7921569 ]\n",
      "   [0.78431374 0.78431374 0.7921569 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9882353  0.9882353  0.99607843]\n",
      "   [0.9882353  0.9882353  0.99607843]\n",
      "   [0.9882353  0.9882353  0.99607843]\n",
      "   ...\n",
      "   [0.85882354 0.85882354 0.8666667 ]\n",
      "   [0.85882354 0.85882354 0.8666667 ]\n",
      "   [0.85882354 0.85882354 0.8666667 ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 1.        ]\n",
      "   [0.9882353  0.9882353  0.99607843]\n",
      "   [0.9843137  0.9843137  0.99215686]\n",
      "   ...\n",
      "   [0.8627451  0.8627451  0.87058824]\n",
      "   [0.8627451  0.8627451  0.87058824]\n",
      "   [0.8627451  0.8627451  0.87058824]]\n",
      "\n",
      "  [[0.9882353  0.9882353  0.99607843]\n",
      "   [0.9882353  0.9882353  0.99607843]\n",
      "   [0.9843137  0.9843137  0.99215686]\n",
      "   ...\n",
      "   [0.8666667  0.8666667  0.8745098 ]\n",
      "   [0.87058824 0.87058824 0.8784314 ]\n",
      "   [0.8666667  0.8666667  0.8745098 ]]]\n",
      "\n",
      "\n",
      " [[[0.8980392  0.8980392  0.90588236]\n",
      "   [0.8980392  0.8980392  0.90588236]\n",
      "   [0.9019608  0.9019608  0.9098039 ]\n",
      "   ...\n",
      "   [0.94509804 0.9411765  0.9607843 ]\n",
      "   [0.9411765  0.9372549  0.95686275]\n",
      "   [0.9372549  0.93333334 0.9529412 ]]\n",
      "\n",
      "  [[0.9019608  0.9019608  0.9098039 ]\n",
      "   [0.8980392  0.8980392  0.90588236]\n",
      "   [0.8980392  0.8980392  0.90588236]\n",
      "   ...\n",
      "   [0.94509804 0.9411765  0.9607843 ]\n",
      "   [0.94509804 0.9411765  0.9607843 ]\n",
      "   [0.9411765  0.9372549  0.95686275]]\n",
      "\n",
      "  [[0.9019608  0.9019608  0.9098039 ]\n",
      "   [0.8980392  0.8980392  0.90588236]\n",
      "   [0.89411765 0.89411765 0.9019608 ]\n",
      "   ...\n",
      "   [0.9490196  0.9490196  0.95686275]\n",
      "   [0.9490196  0.9490196  0.95686275]\n",
      "   [0.9490196  0.9490196  0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9254902  0.9254902  0.9254902 ]\n",
      "   [0.9254902  0.9254902  0.9254902 ]\n",
      "   [0.9254902  0.9254902  0.9254902 ]\n",
      "   ...\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]]\n",
      "\n",
      "  [[0.9254902  0.9254902  0.93333334]\n",
      "   [0.9254902  0.9254902  0.93333334]\n",
      "   [0.9254902  0.9254902  0.93333334]\n",
      "   ...\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]]\n",
      "\n",
      "  [[0.9254902  0.9254902  0.93333334]\n",
      "   [0.9254902  0.9254902  0.93333334]\n",
      "   [0.9254902  0.9254902  0.93333334]\n",
      "   ...\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]\n",
      "   [0.972549   0.972549   0.98039216]]]] ['shapewear' 'tshirts' 'tops' ... 'dresses' 'dresses' 'jeans']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 188\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilar Images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similar_images)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m rec_model \u001b[38;5;241m=\u001b[39m ClothingRecommendationModel()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[43mrec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Dataset/final_dataset_id_local_paths.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m    175\u001b[0m rec_model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclothing_recommendation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m, in \u001b[0;36mClothingRecommendationModel.train\u001b[1;34m(self, csv_path, epochs, batch_size)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(X, y)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    104\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    105\u001b[0m     X_train, y_train,\n\u001b[0;32m    106\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    107\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    108\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[0;32m    109\u001b[0m )\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2876\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2872\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2874\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m-> 2876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2878\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2877\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2872\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2874\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 2877\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2878\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2879\u001b[0m     )\n\u001b[0;32m   2880\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "class ClothingRecommendationModel:\n",
    "    def __init__(self, input_shape=(224, 224, 3), embedding_dim=256):\n",
    "        \"\"\"Initialize clothing recommendation model\"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build recommendation model architecture\"\"\"\n",
    "        # Base model with ResNet50\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Custom embedding layers with ReLU\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Final embedding layer\n",
    "        embedding = layers.Dense(\n",
    "            self.embedding_dim, \n",
    "            activation='relu', \n",
    "            name='embedding_layer'\n",
    "        )(x)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = models.Model(\n",
    "            inputs=base_model.input, \n",
    "            outputs=embedding\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='cosine_similarity'\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def load_and_preprocess_image(self, image_path):\n",
    "        \"\"\"Load and preprocess single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = load_img(image_path, target_size=self.input_shape)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array /= 255.0\n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_dataset(self, csv_path):\n",
    "        \"\"\"Prepare dataset from CSV\"\"\"\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        print(\"CSV Loaded: \", df.head()) \n",
    "        \n",
    "        # Filter valid images\n",
    "        df = df[df['local_path'].apply(os.path.exists)]\n",
    "        \n",
    "        # Prepare images and categories\n",
    "        images = []\n",
    "        categories = []\n",
    "        \n",
    "        for path, category in zip(df['local_path'], df['category']):\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                images.append(img[0])\n",
    "                categories.append(category) \n",
    "        \n",
    "        return np.array(images), np.array(categories)\n",
    "    \n",
    "    def train(self, csv_path, epochs=10, batch_size=32):\n",
    "        \"\"\"Train recommendation model\"\"\"\n",
    "        # Prepare dataset\n",
    "        X, y = self.prepare_dataset(csv_path)\n",
    "        \n",
    "        print(X, y)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def find_similar_images(self, reference_image_path, image_database_df, top_k=5):\n",
    "        \"\"\"Find similar images based on embedding similarity\"\"\"\n",
    "        # Preprocess reference image\n",
    "        ref_img = self.load_and_preprocess_image(reference_image_path)\n",
    "        \n",
    "        if ref_img is None:\n",
    "            return []\n",
    "        \n",
    "        # Get reference embedding\n",
    "        ref_embedding = self.model.predict(ref_img)\n",
    "        \n",
    "        # Prepare database embeddings\n",
    "        db_images = []\n",
    "        db_paths = []\n",
    "        \n",
    "        for path in image_database_df['local_path']:\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                db_images.append(img[0])\n",
    "                db_paths.append(path)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        db_images = np.array(db_images)\n",
    "        \n",
    "        # Compute database embeddings\n",
    "        db_embeddings = self.model.predict(db_images)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = tf.keras.losses.cosine_similarity(\n",
    "            ref_embedding, \n",
    "            db_embeddings\n",
    "        ).numpy()\n",
    "        \n",
    "        # Find top-k similar images\n",
    "        top_indices = np.argsort(similarities[0])[:top_k]\n",
    "        \n",
    "        return [db_paths[i] for i in top_indices]\n",
    "    \n",
    "    def save_model(self, save_path='clothing_recommendation_model'):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.save(os.path.join(save_path, 'full_model'))\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, load_path='clothing_recommendation_model'):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        model_instance = cls()\n",
    "        model_instance.model = tf.keras.models.load_model(\n",
    "            os.path.join(load_path, 'full_model')\n",
    "        )\n",
    "        return model_instance\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize model\n",
    "    rec_model = ClothingRecommendationModel()\n",
    "    \n",
    "    # Train model\n",
    "    rec_model.train('../Dataset/final_dataset_id_local_paths.csv')\n",
    "    \n",
    "    # Save model\n",
    "    rec_model.save_model('clothing_recommendation')\n",
    "    \n",
    "    # Load model (demonstration)\n",
    "    loaded_model = ClothingRecommendationModel.load_model('clothing_recommendation')\n",
    "    \n",
    "    # Find similar images\n",
    "    similar_images = loaded_model.find_similar_images(\n",
    "        'saree.jpeg', \n",
    "        pd.read_csv('clothing_dataset.csv')\n",
    "    )\n",
    "    print(\"Similar Images:\", similar_images)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to clothing_recommendation\\full_model.keras\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=clothing_recommendation\\full_model.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 265\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilar Images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similar_images)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 265\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 254\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    251\u001b[0m rec_model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclothing_recommendation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Load model (demonstration)\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mClothingRecommendationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclothing_recommendation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Find similar images\u001b[39;00m\n\u001b[0;32m    257\u001b[0m similar_images \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mfind_similar_images(\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaree.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    259\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Dataset/final_dataset_id_local_paths.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    260\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 236\u001b[0m, in \u001b[0;36mClothingRecommendationModel.load_model\u001b[1;34m(cls, load_path)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load saved model\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m--> 236\u001b[0m model_instance\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_model.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_instance\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=clothing_recommendation\\full_model.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "class ClothingRecommendationModel:\n",
    "    def __init__(self, input_shape=(224, 224, 3), embedding_dim=256):\n",
    "        \"\"\"Initialize clothing recommendation model\"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_encoder = None  # For label encoding\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build recommendation model architecture\"\"\"\n",
    "        # Base model with ResNet50\n",
    "        base_model = ResNet50(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "\n",
    "        # Freeze base model layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Custom embedding layers with ReLU\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        # Final embedding layer\n",
    "        embedding = layers.Dense(\n",
    "            self.embedding_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"embedding_layer\"\n",
    "        )(x)\n",
    "\n",
    "        # Create and compile model\n",
    "        model = models.Model(\n",
    "            inputs=base_model.input,\n",
    "            outputs=embedding\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss=\"cosine_similarity\"\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_and_preprocess_image(self, image_path):\n",
    "        \"\"\"Load and preprocess single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = load_img(image_path, target_size=self.input_shape[:2])\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array /= 255.0\n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self, csv_path):\n",
    "        \"\"\"Prepare dataset from CSV\"\"\"\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Filter valid images\n",
    "        df = df[df[\"local_path\"].apply(os.path.exists)]\n",
    "\n",
    "        # Prepare images and categories\n",
    "        images = []\n",
    "        categories = []\n",
    "\n",
    "        for path, category in tqdm(\n",
    "            zip(df[\"local_path\"], df[\"category\"]),\n",
    "            total=len(df),\n",
    "            desc=\"Preparing dataset\"\n",
    "        ):\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                images.append(img[0])  # Add image shape correction\n",
    "                categories.append(category)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        images = np.array(images)\n",
    "\n",
    "        # Encode categories into numeric labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        categories = self.label_encoder.fit_transform(categories)  # Numeric encoding\n",
    "\n",
    "        return images, categories\n",
    "\n",
    "    # def train(self, csv_path, epochs=10, batch_size=16):\n",
    "    #     \"\"\"Train recommendation model\"\"\"\n",
    "    #     # Prepare dataset\n",
    "    #     X, y = self.prepare_dataset(csv_path)\n",
    "\n",
    "    #     # Split data\n",
    "    #     X_train, X_val, y_train, y_val = train_test_split(\n",
    "    #         X, y, test_size=0.2, random_state=42\n",
    "    #     )\n",
    "\n",
    "    #     # Use categorical crossentropy instead of cosine similarity\n",
    "    #     self.model.compile(\n",
    "    #         optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    #         loss='sparse_categorical_crossentropy',\n",
    "    #         metrics=['accuracy']\n",
    "    #     )\n",
    "\n",
    "    #     print(\"Training model:\")\n",
    "    #     # Train model\n",
    "    #     history = self.model.fit(\n",
    "    #         X_train, y_train,\n",
    "    #         validation_data=(X_val, y_val),\n",
    "    #         epochs=epochs,\n",
    "    #         batch_size=batch_size,\n",
    "    #         verbose=1  # Use Keras's built-in progress bar\n",
    "    #     )\n",
    "\n",
    "    #     return history\n",
    "\n",
    "    def train(self, csv_path, epochs=10, batch_size=16):\n",
    "        \"\"\"Train recommendation model\"\"\"\n",
    "        # Prepare dataset\n",
    "        X, y = self.prepare_dataset(csv_path)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"Training model:\")\n",
    "        try:\n",
    "            # Train model with early stopping\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=3, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            return history\n",
    "        except Exception as e:\n",
    "            print(f\"Training interrupted: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def find_similar_images(self, reference_image_path, image_database_df, top_k=5):\n",
    "        \"\"\"Find similar images based on embedding similarity\"\"\"\n",
    "        # Preprocess reference image\n",
    "        ref_img = self.load_and_preprocess_image(reference_image_path)\n",
    "\n",
    "        if ref_img is None:\n",
    "            return []\n",
    "\n",
    "        # Get reference embedding\n",
    "        ref_embedding = self.model.predict(ref_img)\n",
    "\n",
    "        # Prepare database embeddings\n",
    "        db_images = []\n",
    "        db_paths = []\n",
    "\n",
    "        for path in tqdm(\n",
    "            image_database_df[\"local_path\"],\n",
    "            desc=\"Processing database images\"\n",
    "        ):\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                db_images.append(img[0])\n",
    "                db_paths.append(path)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        db_images = np.array(db_images)\n",
    "\n",
    "        # Compute database embeddings\n",
    "        db_embeddings = self.model.predict(db_images)\n",
    "\n",
    "        # Compute similarities\n",
    "        ref_embedding = np.expand_dims(ref_embedding, axis=0)  # Ensure proper shape\n",
    "        similarities = tf.keras.losses.cosine_similarity(\n",
    "            ref_embedding,\n",
    "            db_embeddings\n",
    "        ).numpy()\n",
    "\n",
    "        # Find top-k similar images\n",
    "        top_indices = np.argsort(similarities[0])[:top_k]\n",
    "\n",
    "        return [db_paths[i] for i in top_indices]\n",
    "\n",
    "    def save_model(self, save_path=\"clothing_recommendation_model\"):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        # Add full filepath with .keras extension\n",
    "        full_filepath = os.path.join(save_path, \"full_model.keras\")\n",
    "        self.model.save(\"full_model.keras\")\n",
    "        print(f\"Model saved to {full_filepath}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, load_path=\"clothing_recommendation_model\"):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        model_instance = cls()\n",
    "        model_instance.model = tf.keras.models.load_model(\n",
    "            os.path.join(load_path, \"full_model.keras\")\n",
    "        )\n",
    "        return model_instance\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize model\n",
    "    rec_model = ClothingRecommendationModel()\n",
    "\n",
    "    # Train model\n",
    "    # rec_model.train(\"../Dataset/final_dataset_id_local_paths.csv\")\n",
    "\n",
    "    # Save model\n",
    "    rec_model.save_model(\"clothing_recommendation\")\n",
    "\n",
    "    # Load model (demonstration)\n",
    "    loaded_model = ClothingRecommendationModel.load_model(\"clothing_recommendation\")\n",
    "\n",
    "    # Find similar images\n",
    "    similar_images = loaded_model.find_similar_images(\n",
    "        \"saree.jpeg\",\n",
    "        pd.read_csv(\"../Dataset/final_dataset_id_local_paths.csv\")\n",
    "    )\n",
    "    print(\"Similar Images:\", similar_images)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=clothing_recommendation\\full_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mrec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclothing_recommendation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 227\u001b[0m, in \u001b[0;36mClothingRecommendationModel.save_model\u001b[1;34m(self, save_path)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save trained model\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 227\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Python\\envs\\ml\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    113\u001b[0m     )\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=clothing_recommendation\\full_model."
     ]
    }
   ],
   "source": [
    "rec_model = ClothingRecommendationModel()\n",
    "\n",
    "def save_model(self, save_path=\"clothing_recommendation_model\"):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        # Add full filepath with .keras extension\n",
    "        full_filepath = os.path.join(save_path, \"full_model.keras\")\n",
    "        self.model.save(\"full_model.keras\")\n",
    "        print(f\"Model saved to {full_filepath}\")\n",
    "\n",
    "rec_model.save_model(\"clothing_recommendation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\envs\\ml\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ClothingRecommendationModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a new ClothingRecommendationModel instance\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m rec_model \u001b[38;5;241m=\u001b[39m \u001b[43mClothingRecommendationModel\u001b[49m()\n\u001b[0;32m     18\u001b[0m rec_model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m loaded_model\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Now use the loaded model for recommendations\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ClothingRecommendationModel' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(os.path.join(\"full_model.keras\"))\n",
    "\n",
    "# Create a new ClothingRecommendationModel instance\n",
    "rec_model = ClothingRecommendationModel()\n",
    "rec_model.model = loaded_model\n",
    "\n",
    "# Now use the loaded model for recommendations\n",
    "similar_images = rec_model.find_similar_images(\n",
    "    'saree.jpeg', \n",
    "    pd.read_csv(\"../Dataset/final_dataset_id_local_paths.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Images: ['processed_images/4899.jpg', 'processed_images/8185.jpg', 'processed_images/8064.jpg', 'processed_images/5863.jpg', 'processed_images/986.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar Images:\", similar_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX\u001b[49m, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothingRecommendationModel:\n",
    "    def __init__(self, input_shape=(224, 224, 3), embedding_dim=256):\n",
    "        \"\"\"Initialize clothing recommendation model\"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build recommendation model architecture\"\"\"\n",
    "        # Base model with ResNet50\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Custom embedding layers with ReLU\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Final embedding layer\n",
    "        embedding = layers.Dense(\n",
    "            self.embedding_dim, \n",
    "            activation='relu', \n",
    "            name='embedding_layer'\n",
    "        )(x)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = models.Model(\n",
    "            inputs=base_model.input, \n",
    "            outputs=embedding\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='cosine_similarity'\n",
    "        )\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(self, image_path):\n",
    "    \"\"\"Load and preprocess single image\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = load_img(image_path, target_size=self.input_shape)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(self, csv_path):\n",
    "    \"\"\"Prepare dataset from CSV\"\"\"\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    print(\"CSV Loaded: \", df.head()) \n",
    "    \n",
    "    # Filter valid images\n",
    "    df = df[df['local_path'].apply(os.path.exists)]\n",
    "\n",
    "    print(\"df in preprare dataset\",df)\n",
    "    \n",
    "    # Prepare images and categories\n",
    "    images = []\n",
    "    categories = []\n",
    "    \n",
    "    for path, category in zip(df['local_path'], df['category']):\n",
    "        img = self.load_and_preprocess_image(path)\n",
    "        if img is not None:\n",
    "            images.append(img[0])\n",
    "            categories.append(category) \n",
    "    \n",
    "    return np.array(images), np.array(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, csv_path, epochs=10, batch_size=32):\n",
    "    \"\"\"Train recommendation model\"\"\"\n",
    "    # Prepare dataset\n",
    "    X, y = self.prepare_dataset(csv_path)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = self.model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(self, reference_image_path, image_database_df, top_k=5):\n",
    "    \"\"\"Find similar images based on embedding similarity\"\"\"\n",
    "    # Preprocess reference image\n",
    "    ref_img = self.load_and_preprocess_image(reference_image_path)\n",
    "    \n",
    "    if ref_img is None:\n",
    "        return []\n",
    "    \n",
    "    # Get reference embedding\n",
    "    ref_embedding = self.model.predict(ref_img)\n",
    "    \n",
    "    # Prepare database embeddings\n",
    "    db_images = []\n",
    "    db_paths = []\n",
    "    \n",
    "    for path in image_database_df['local_path']:\n",
    "        img = self.load_and_preprocess_image(path)\n",
    "        if img is not None:\n",
    "            db_images.append(img[0])\n",
    "            db_paths.append(path)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    db_images = np.array(db_images)\n",
    "    \n",
    "    # Compute database embeddings\n",
    "    db_embeddings = self.model.predict(db_images)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = tf.keras.losses.cosine_similarity(\n",
    "        ref_embedding, \n",
    "        db_embeddings\n",
    "    ).numpy()\n",
    "    \n",
    "    # Find top-k similar images\n",
    "    top_indices = np.argsort(similarities[0])[:top_k]\n",
    "    \n",
    "    return [db_paths[i] for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, save_path='clothing_recommendation_model'):\n",
    "    \"\"\"Save trained model\"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    self.model.save(os.path.join(save_path, 'full_model'))\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "@classmethod\n",
    "def load_model(cls, load_path='clothing_recommendation_model'):\n",
    "    \"\"\"Load saved model\"\"\"\n",
    "    model_instance = cls()\n",
    "    model_instance.model = tf.keras.models.load_model(\n",
    "        os.path.join(load_path, 'full_model')\n",
    "    )\n",
    "    return model_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def main():\n",
    "    # Initialize model\n",
    "    rec_model = ClothingRecommendationModel()\n",
    "    \n",
    "    # Train model\n",
    "    rec_model.train('../Dataset/final_dataset_id_local_paths.csv')\n",
    "    \n",
    "    # Save model\n",
    "    rec_model.save_model('clothing_recommendation')\n",
    "    \n",
    "    # Load model (demonstration)\n",
    "    loaded_model = ClothingRecommendationModel.load_model('clothing_recommendation')\n",
    "    \n",
    "    # Find similar images\n",
    "    similar_images = loaded_model.find_similar_images(\n",
    "        'saree.jpeg', \n",
    "        pd.read_csv('clothing_dataset.csv')\n",
    "    )\n",
    "    print(\"Similar Images:\", similar_images)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, csv_path, epochs=10, batch_size=16):\n",
    "        # Prepare dataset\n",
    "        X, y = self.prepare_dataset(csv_path)\n",
    "\n",
    "        # Create data generator\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Recompile model with appropriate loss function\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='sparse_categorical_crossentropy',  # Change loss function\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        history = self.model.fit(                                                                                  \n",
    "            datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
